---
title: "Analysis_bulls"
author: "Sara G"
date: "27/05/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Opening libraries

```{r librarys}
library(tidyverse)
library(dplyr)
library(broom)
```


# Reading in and tidying Bulls data files

```{r read_data, message=TRUE} 
stat_tm1 <- read_csv("data/raw/2018-19_nba_team_statistics_1.csv")
stat_tm2 <- read_csv("data/raw/2018-19_nba_team_statistics_2.csv")
```

# Tidying data

## Binding data from two data frames
Binding variables from team 1 data and team 2 data

```{r binding}
df_team <- left_join(x = stat_tm2, y = stat_tm1 [-c(1,3, 23:25)],
                     by = "Team")
```


## renaming variables: 
# Making sure variables do not include a %, replace with 'p'. And added a 'x' in front of number variables. Change '/' to *_per_* .

```{r renaming}
df_team <- df_team %>%
  rename( x3P = '3P', x3PA = '3PA', x3Pp = '3P%', x2P = '2P', x2PA = '2PA', x2Pp = '2P%', FGp = 'FG%', FTp = 'FT%', x3PAr = "3PAr", TSp = "TS%", eFGp = "eFG%", TOVp = "TOV%", ORBp = "ORB%", FT_per_FGA = "FT/FGA", DRBp = "DRB%")
```

## Normalising key metrics

We want normalise the key metrics to per game

```{r Normalising metrics_pg}
df_team <- df_team %>%
  mutate (PTS_per_game = PTS / G,
          WINp = W/G,
          AST_per_game = AST / G,
          FG_per_game = FGA / G, 
          x3P_per_game = x3P / G,
          x2P_per_game = x2PA / G,
          FT_per_game = FTA / G,
          STL_per_game = STL / G,
          BLK_per_game = BLK / G,
          TOV_per_game = TOV / G,
          ORB_per_game = ORB / G,
          DRB_per_game = DRB / G,
          TRB_per_game = TRB / G,
          PF_per_game = PF / G)
```

## Normalising variables per minuted played minutes played
There are 5 players playing roughly 48mins per game. Therefore there are ~240mins per game. WE want normalise these varaibles for the minutes played. 

```{r Normalising metrics_pmp}
df_team <- df_team %>%
  mutate (PTS_pmp = PTS / MP , 
          AST_pmp = AST / MP,
          FG_pmp = FGA / MP, 
          x3P_pmp = x3P / MP ,
          x2P_pmp = x2P / MP ,
          FT_pmp = FTA / MP,
          STL_pmp = STL / MP ,
          BLK_pmp = BLK / MP,
          TOV_pmp = TOV / MP,
          ORB_pmp = ORB / MP,
          DRB_pmp = DRB / MP,
          TRB_pmp = TRB / MP,
          PF_pmp = PF / MP)
        
```

##Check structure of new data set

```{r Str_df_team}
str(df_team)
```


## Multiple Regression
###Checking confidence intervals, R squared stat and intercept to see which variables have a relationship and which don't.

### Wasn't sure whether to use per game played or per minute played as this is a more common stat in Basketball. So looked at both.

# 1. Per Game metric 

```{r MLR per game}
lm_teams <- lm(PTS_per_game ~ AST_per_game + FG_per_game + x3P_per_game + x2P_per_game + FT_per_game + STL_per_game + BLK_per_game + TOV_per_game + DRB_per_game + ORB_per_game, data = df_team)
tidy(lm_teams, conf.int = TRUE)
```

```{r summary}
summary(lm_teams)
```

# 2. Per minute played metric 

```{r lm mp}
lm_mp <- lm(PTS_pmp ~ AST_pmp + FG_pmp + x3P_pmp + x2P_pmp + FT_pmp + STL_pmp + BLK_pmp + TOV_pmp + DRB_pmp + ORB_pmp, data = df_team)
tidy(lm_mp, conf.int = TRUE)
```

```{r summary lm mp}
summary(lm_mp)
```


## Based on the linear regression confidence intervals that are crossing over zero (therefore stating that there isn't a relationship). 
## The linear regression model has been Updated to only include variables with a relationship.

*per game*

```{r new lm pg}
fit_pg <- lm(PTS_per_game ~ AST_per_game + x3P_per_game + x2P_per_game + FT_per_game, data = df_team)
tidy(fit_pg, conf.int = TRUE)
```

```{r summary pg}
summary(fit_pg)
```

*per minute played*

```{r new lm wmp}
fit_mp <- lm(PTS_pmp ~ x3P_pmp + x2P_pmp + FT_pmp, data = df_team)
tidy(fit_mp, conf.int = TRUE)
```


```{r summary per minute played}
summary(fit_mp)
```

## Checking assumptions for MLR for both per game and minutes played

1, Response variables should be continuous. Yes this is met
2. There are wo or more explanatory variables that are continuous or categorical Yes all of variables are continous
3. Independence

```{r Assumpt_independence}
car::durbinWatsonTest(fit_pg)
```

```{r Assumpt_independence_mp}
car::durbinWatsonTest(fit_mp)
```
## Both of these D- W statistics are close to 2 therefore they pass the assumption

4. Linearity

```{r linearity}
car::avPlots(fit_pg)
```

# a positive relationship can be seen with these explanatory variables in relation to gaining points in a basketball game

```{r linearity}
car::avPlots(fit_mp)
  
```
# a positive relationship can be seen with these explanatory variables in relation to gaining points in a basketball game

5. Outliers, Influential or high leverage points

```{r outliers _PG}
std_res <- rstandard(fit_pg)
points <- 1:length(std_res)

ggplot(data = NULL, aes(x = points, y = std_res)) +
  geom_point() +
  ylim(c(-4,4)) +
  geom_hline(yintercept = c(-3,3), colour = "red", linetype = "dashed")
```

```{r outliers labeled pg}
res_labels <- if_else(abs(std_res) >= 2, paste(points), "")
                    
ggplot(data = NULL, aes(x = points, y = std_res, label = res_labels)) +
  geom_point() +
  ylim(c(-4,4)) +
  geom_text(nudge_x = 0.5 ) +
  geom_hline(yintercept = c(-3,3), colour = "red", linetype = "dashed")
```



```{r outliers labeled mp}
std_res_mp <- rstandard(fit_mp)
points_mp <- 1:length(std_res_mp)
res_labels_mp <- if_else(abs(std_res_mp) >= 2, paste(points_mp), "")
                    
ggplot(data = NULL, aes(x = points_mp, y = std_res_mp, label = res_labels_mp)) +
  geom_point() +
  ylim(c(-4,4)) +
  geom_text(nudge_x = 0.5 ) +
  geom_hline(yintercept = c(-3,3), colour = "red", linetype = "dashed")
```

## Overall there are minimal outliers for both per game and per minutes played. But we will investigate the leverage and inflence of these points below. We will then decide whether we should modify the model to eliminate the outlier points with high influence/leverage. 

## Leverage points

```{r hats PG}
hats <- hatvalues(fit_pg)
hats_labels <- if_else(hats >= 0.2, paste(points), "")

ggplot(data = NULL, aes(x = points, y = hats)) +
  geom_point() +
  geom_text(aes(label = hats_labels), nudge_y = 0.02)
```


# There are no hat values that are greater than 1 but should investigate the hat values that are greater than 0.2 as they are above the rest


```{r hats MP}
hats_mp <- hatvalues(fit_mp)
hats_labels_mp <- if_else(hats_mp >= 0.2, paste(points), "")

ggplot(data = NULL, aes(x = points_mp, y = hats_mp)) +
  geom_point() +
  geom_text(aes(label = hats_labels_mp), nudge_y = 0.02)
```


# There are no hat values that are greater than 1 but should investigate the hat values that are greater than 0.2 as they are above the rest

## Influential points

```{r Cook PG}
cook <- cooks.distance(fit_pg)
cook_labels <- if_else(cook >= 0.1, paste(points), "")

ggplot(data = NULL, aes(x = points, y = cook)) +
  geom_point() +
  geom_text(aes(label = cook_labels), nudge_y = 0.02)

```

```{r Cook MP}
cook_mp <- cooks.distance(fit_mp)
cook_labels_mp <- if_else(cook_mp >= 0.075, paste(points), "")

ggplot(data = NULL, aes(x = points_mp, y = cook_mp)) +
  geom_point() +
  geom_text(aes(label = cook_labels_mp), nudge_y = 0.01)

```

## There are no points that are above 1, but we will look at points above 0.1
## There are a few points that have high influence in the minutesplayed data frame so we will test out the model without the outliers. 


## We will create a new df without the high influence points for minutes played as this is the model we most likely will use.

```{r new df with no outliers_ MP}
outliers <- c(5, 11, 16, 26 )
filtered_df <- df_team %>%
  filter(!Rk %in% outliers)

```


```{r new lm with filtered_df_mp}
fit2_mp <- lm(PTS_pmp ~ x3P_pmp + x2P_pmp + FT_pmp, data = filtered_df)

tidy(fit2_mp, conf.int = TRUE)
```

```{r summary filtered_df_mp}
summary(fit2_mp)
```

## by filtering out these variables there is minimal change to the slope or the intercept or the R-square value (from 0.96 to 0.98).
## Removing these points also changes the intercept and the slope coefficent but only by a little. But as removing values adds in bias to the data, these points will not be removed. 

## WE will continue checking the rest of the assumptions

6. Homoscedasticity

```{r homoscedasticity}
res <- residuals(fit_pg)
fitted <- predict(fit_pg)

ggplot(data = NULL, aes(x = fitted, y = res)) +
  geom_point(colour = "dodgerblue") +
  geom_hline(yintercept = 0, colour = "red", linetype = "dashed")

```

#There is no evidence of heteroscedasticity


```{r homoscedasticity}
res_mp <- residuals(fit_mp)
res_mp2 <- residuals(fit2_mp)
fitted_mp <- predict(fit_mp)
fitted_mp2 <- predict(fit2_mp)

ggplot(data = NULL, aes(x = fitted_mp, y = res_mp)) +
  geom_point(colour = "dodgerblue") +
  geom_hline(yintercept = 0, colour = "red", linetype = "dashed")

```

#There is no evidence of heteroscedasticity

7. Normality

```{r normality_PG}
ggplot(data = NULL, aes(x = res)) +
  geom_histogram(colour = "black", fill = "dodgerblue", binwidth = 0.5)
```

#Does not look normally distributed, try minutes played and check Q-Q plot for both

```{r normality_MP}
ggplot(data = NULL, aes(x = res_mp)) +
  geom_histogram(colour = "black", fill = "dodgerblue", binwidth = 0.0045)
```

# Looks normally distributed. There are tails on each end. However the L tail is a little longer. Check Q-Q plot

```{r qq plot PG}
ggplot(data = NULL, aes(sample = res)) +
  stat_qq() + stat_qq_line()
         
```
## Most of the data points are near the reference line. SLight tail on the L side. Check minutes played


```{r qq plot PG}
ggplot(data = NULL, aes(sample = res_mp)) +
  stat_qq() + stat_qq_line() +
  ylim(c(-0.01, 0.01))
         
```
## The tail is a little longer near the bottom end but the points are more on the line than the per game line


8. Multicollinearity

```{r multicollinearity PG}
pairs(formula= ~ AST_per_game + x3P_per_game + x2P_per_game + FT_per_game, data = df_team)
```

## Might be a relationship between 3P and 2p?

```{r VIF PG}
car::vif(fit_pg)
```

#There is a lot of multicollinearity seen. There is large variation between 3P and 2P as they are both far away from 1. 

```{r multicollinearity MP}
pairs(formula= ~ x3P_pmp + x2P_pmp + FT_pmp, data = df_team)
```

# No relationship seen with minutes played

```{r VIF MP}
car::vif(fit_mp)
```
## These values are a lot closer to 1 than when normalised to per game
## So based on this we have chosen 'fit_mp' as our predicted model.

# Predicted values.
## Check predicted model for minutes played
```{r predict MP}
head(predict(fit_mp))
```

```{r summary MP}
summary(fit_mp)
```

##Predict model testing
## Use the predicted wins (PW) based on the data from Stats team

## Find new expected model metric and find coefficient and intercept to help with reference line on plot below
```{r predicted wins ceofficient}
exp_PTS_pmp = predict(fit_mp)
coef(lm(exp_PTS_pmp ~ PW, data = df_team)) # used this to find the slope and coefficient of these two variables

```

```{r predicted wins MP}
 ggplot(df_team, aes(PW,exp_PTS_pmp, label = Team)) +
  geom_point(colour = "dodgerblue") +
  geom_text(nudge_x = 0.005, cex = 3) +
  geom_abline(linetype = "dashed", colour = "red", intercept = 0.421535, slope = 0.000941072  )

```
Can see that with some teams above the line the model underestimated the points, and teams below the line the model overestimated the points needed to win. 

# Against actual win percentage

```{r predicted wins ceofficient}
exp_PTS_pmp = predict(fit_mp)
coef(lm(exp_PTS_pmp ~ W, data = df_team)) # used this to find the slope and coefficient of these two variables

```

```{r predicted points vs wins MP}
gg_team_model_prediction <- ggplot(df_team, aes((W/82*100), exp_PTS_pmp, label = Team)) +
  geom_point(colour = "dodgerblue") +
  geom_text(nudge_x = 0.005, nudge_y = 0.001, cex = 2.5) +
  geom_abline(linetype = "dashed", colour = "red", intercept = 0.42228755184, slope = 0.000910  )
gg_team_model_prediction
```
## This model compared the actual win percentage fromt the 2018-2019 season. The model is on the Y axis now. WE can see that the teams on the upper end were closer to the reference line and therefore these teams seem to have a linear relationship. There are a few outliers on the lower side of the line where they scored a lot less runs than their win percentage. 


###For the majoirty of teams this metric shows that expected points per minute will correlate with a win. 
## WE will use it for the player specific analysis now. SEe the file that is Bulls_Player_Metric to see the next step in the analysis. 


