---
title: "Analysis_bulls"
author: "Sara G"
date: "27/05/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Opening libraries

```{r librarys}
library(tidyverse)
library(dplyr)
library(broom)
```


# Reading in and tidying Bulls data files

```{r read_data, message=TRUE} 
sal <- read_csv("data/raw/2018-19_nba_player-salaries.csv")
stat_pl <- read_csv("data/raw/2018-19_nba_player_statistics.csv")
stat_tm1 <- read_csv("data/raw/2018-19_nba_team_statistics_1.csv")
stat_tm2 <- read_csv("data/raw/2018-19_nba_team_statistics_2.csv")
pay <- read_csv("data/raw/2019-20_nba_team-payroll.csv")
```

# Tidying data

## Binding data from two data frames
Binding variables from team 1 data and team 2 data

```{r binding}
df_team <- left_join(x = stat_tm2, y = stat_tm1 [-c(1,3, 23:25)],
                     by = "Team")
```

## renaming variables: 
Making sure variables do not include a %, replace with 'p'. And added a 'x' in front of number variables. Change '/' to _per_ .

```{r renaming}
df_team <- df_team %>%
  rename( x3P = '3P', x3PA = '3PA', x3Pp = '3P%', x2P = '2P', x2PA = '2PA', x2Pp = '2P%', FGp = 'FG%', FTp = 'FT%', x3PAr = "3PAr", TSp = "TS%", eFGp = "eFG%", TOVp = "TOV%", ORBp = "ORB%", FT_per_FGA = "FT/FGA", DRBp = "DRB%")
```

## Normalising key metrics

We want normalise the key metrics to per game

```{r Normalising metrics_pg}
df_team <- df_team %>%
  mutate (PTS_per_game = PTS / G,
          WINp = W/G,
          AST_per_game = AST / G,
          FG_per_game = FGA / G, 
          x3P_per_game = x3P / G,
          x2P_per_game = x2PA / G,
          FT_per_game = FTA / G,
          STL_per_game = STL / G,
          BLK_per_game = BLK / G,
          TOV_per_game = TOV / G,
          ORB_per_game = ORB / G,
          DRB_per_game = DRB / G,
          TRB_per_game = TRB / G,
          PF_per_game = PF / G)
```

## Normalising variables per minuted played minutes played
There are 5 players playing roughly 48mins per game. Therefore there are ~240mins per game. WE want normalise these varaibles for the minutes played. 

```{r Normalising metrics_pmp}
df_team <- df_team %>%
  mutate (PTS_pmp = PTS / MP , 
          AST_pmp = AST / MP,
          FG_pmp = FGA / MP, 
          x3P_pmp = x3P / MP ,
          x2P_pmp = x2P / MP ,
          FT_pmp = FTA / MP,
          STL_pmp = STL / MP ,
          BLK_pmp = BLK / MP,
          TOV_pmp = TOV / MP,
          ORB_pmp = ORB / MP,
          DRB_pmp = DRB / MP,
          TRB_pmp = TRB / MP,
          PF_pmp = PF / MP)
        
```

##Check structure of new data set

```{r Str_df_team}
str(df_team)
```



## Multiple Regression
###Checking confidence intervals, R squared stat and intercept to see which variables have a relationship and which don't.

### Wasn't sure whether to use per game played or per minute played as this is a more common stat in Basketball. So looked at both.

# 1. Per Game metric 

```{r MLR per game}
lm_teams <- lm(PTS_per_game ~ AST_per_game + FG_per_game + x3P_per_game + x2P_per_game + FT_per_game + STL_per_game + BLK_per_game + TOV_per_game + DRB_per_game + ORB_per_game, data = df_team)
tidy(lm_teams, conf.int = TRUE)
```

```{r summary}
summary(lm_teams)
```

# 2. Per minute played metric 

```{r lm mp}
lm_mp <- lm(PTS_pmp ~ AST_pmp + FG_pmp + x3P_pmp + x2P_pmp + FT_pmp + STL_pmp + BLK_pmp + TOV_pmp + DRB_pmp + ORB_pmp, data = df_team)
tidy(lm_mp, conf.int = TRUE)
```

```{r summary lm mp}
summary(lm_mp)
```

## Checking assumptions for MLR
### Checking assumptions for linear model  (lm_teams_pg) (lm_mp)

1, Response variables should be continuous. Yes this is met
2. There are wo or more explanatory variables that are continuous or categorical Yes all of variables are continous
3. Independence

```{r Assumpt_independence}
car::durbinWatsonTest(lm_teams)
```

```{r Assumpt_independence_mp}
car::durbinWatsonTest(lm_mp)
```

## D-W statistic is Close to '2' so meets this assumption

4. Linearity

```{r linearity_pg}
car::avPlots(lm_teams)
```

```{r linearity_mp}
car::avPlots(lm_mp)
```



## based on the linear regression confidence intervals that are crossing over zero (therefore stating that there isn't a relationship). And the sign that a few variables do not show a linear relationship:
## The linear regression model has been Updated to only include variables with a relationship.

*per game*

```{r new lm pg}
fit_pg <- lm(PTS_per_game ~ AST_per_game + x3P_per_game + x2P_per_game + FT_per_game, data = df_team)
tidy(fit_pg, conf.int = TRUE)
```

```{r summary pg}
summary(fit_pg)
```

*per minute played*

```{r new lm wmp}
fit_mp <- lm(PTS_pmp ~   x3P_pmp + x2P_pmp + FT_pmp, data = df_team)
tidy(fit_mp, conf.int = TRUE)
```

```{r summary per minute played}
summary(fit_mp)
```

3. Independence

```{r Assumpt_independence}
car::durbinWatsonTest(fit_pg)
```

```{r Assumpt_independence_mp}
car::durbinWatsonTest(fit_mp)
```


4. Linearity

```{r linearity}
car::avPlots(fit_pg)
```

```{r linearity}
car::avPlots(fit_mp)
```


5. Outliers, Influential or high leverage points

```{r outliers}
std_res <- rstandard(fit_pg)
points <- 1:length(std_res)

ggplot(data = NULL, aes(x = points, y = std_res)) +
  geom_point() +
  ylim(c(-4,4)) +
  geom_hline(yintercept = c(-3,3), colour = "red", linetype = "dashed")
```

```{r outliers labeled pg}
res_labels <- if_else(abs(std_res) >= 2, paste(points), "")
                    
ggplot(data = NULL, aes(x = points, y = std_res, label = res_labels)) +
  geom_point() +
  ylim(c(-4,4)) +
  geom_text(nudge_x = 0.5 ) +
  geom_hline(yintercept = c(-3,3), colour = "red", linetype = "dashed")
```



```{r outliers labeled mp}
std_res_mp <- rstandard(fit_mp)
points_mp <- 1:length(std_res_mp)
res_labels_mp <- if_else(abs(std_res_mp) >= 2, paste(points_mp), "")
                    
ggplot(data = NULL, aes(x = points_mp, y = std_res_mp, label = res_labels_mp)) +
  geom_point() +
  ylim(c(-4,4)) +
  geom_text(nudge_x = 0.5 ) +
  geom_hline(yintercept = c(-3,3), colour = "red", linetype = "dashed")
```



## Leverage points

```{r hats PG}
hats <- hatvalues(fit_pg)
hats_labels <- if_else(hats >= 0.2, paste(points), "")

ggplot(data = NULL, aes(x = points, y = hats)) +
  geom_point() +
  geom_text(aes(label = hats_labels), nudge_y = 0.02)
```

```{r hats MP}
hats_mp <- hatvalues(fit_mp)
hats_labels_mp <- if_else(hats_mp >= 0.3, paste(points), "")

ggplot(data = NULL, aes(x = points_mp, y = hats_mp)) +
  geom_point() +
  geom_text(aes(label = hats_labels_mp), nudge_y = 0.02)
```


# There are no hat values that are greater than 1 but should investigate the hat values that are greater than 0.2 as they are above the rest

## Influential points

```{r Cook PG}
cook <- cooks.distance(fit_pg)
cook_labels <- if_else(cook >= 0.1, paste(points), "")

ggplot(data = NULL, aes(x = points, y = cook)) +
  geom_point() +
  geom_text(aes(label = cook_labels), nudge_y = 0.02)

```

```{r Cook MP}
cook_mp <- cooks.distance(fit_mp)
cook_labels_mp <- if_else(cook_mp >= 0.075, paste(points), "")

ggplot(data = NULL, aes(x = points_mp, y = cook_mp)) +
  geom_point() +
  geom_text(aes(label = cook_labels_mp), nudge_y = 0.01)

```

# There are no points that are above 1, but we will look at points above 0.1

We will create a new df without the high influence points

```{r new df with no outliers_ MP}
outliers <- c(5, 11, 16, 26 )
filtered_df <- df_team %>%
  filter(!Rk %in% outliers)

```


```{r new lm with filtered_df}
fit2 <- lm(PTS_per_game ~ AST_per_game + x3P_per_game + x2P_per_game + FT_per_game, data = filtered_df)

tidy(fit2, conf.int = TRUE)
```


# by filtering out these variables there is minimal change to the slope or the intercept or the R-square value (from 0.89 to 0.88).
# we don't want to take out these factors as they can introduce variability to the model and they make minimal change so we will leave them in. 


```{r new lm with filtered_df_mp}
fit2_mp <- lm(PTS_pmp ~ x3P_pmp + x2P_pmp + FT_pmp, data = filtered_df)

tidy(fit2_mp, conf.int = TRUE)
```



```{r summary filtered_df_mp}
summary(fit2_mp)
```

# by filtering out these variables there is minimal change to the slope or the intercept or the R-square value (from 0.96 to 0.98).
# Removing these points also changes the intercept and the slope coefficent but only by a little. But as removing values adds in bias to the data, these points will not be removed. 

6. Homoscedasticity

```{r homoscedasticity}
res <- residuals(fit_pg)
fitted <- predict(fit_pg)

ggplot(data = NULL, aes(x = fitted, y = res)) +
  geom_point(colour = "dodgerblue") +
  geom_hline(yintercept = 0, colour = "red", linetype = "dashed")

```

#There is no evidence of heteroscedasticity


```{r homoscedasticity}
res_mp <- residuals(fit_mp)
res_mp2 <- residuals(fit2_mp)
fitted_mp <- predict(fit_mp)
fitted_mp2 <- predict(fit2_mp)

ggplot(data = NULL, aes(x = fitted_mp, y = res_mp)) +
  geom_point(colour = "dodgerblue") +
  geom_hline(yintercept = 0, colour = "red", linetype = "dashed")

```

#There is no evidence of heteroscedasticity

7. Normality

```{r normality_PG}
ggplot(data = NULL, aes(x = res)) +
  geom_histogram(colour = "black", fill = "dodgerblue", binwidth = 0.5)
```

#Does not look normally distributed, use minutes played

```{r normality_MP}
ggplot(data = NULL, aes(x = res_mp)) +
  geom_histogram(colour = "black", fill = "dodgerblue", binwidth = 0.0045)
```

# Looks normally distributed. There are tails on each end. However the L tail is a little longer

```{r qq plot PG}
ggplot(data = NULL, aes(sample = res)) +
  stat_qq() + stat_qq_line()
         
```

```{r qq plot PG}
ggplot(data = NULL, aes(sample = res_mp)) +
  stat_qq() + stat_qq_line() +
  ylim(c(-0.01, 0.01))
         
```

```{r qq plot PG}
ggplot(data = NULL, aes(sample = res_mp)) +
  stat_qq() + stat_qq_line()
         
```

Transformed data from 5 variables: AST, FG, 3P, 2P, FT, to only 3 variables. 3P, 2P, FT. 

8. Multicollinearity

```{r multicollinearity PG}
pairs(formula= ~ AST_per_game + x3P_per_game + x2P_per_game + FT_per_game, data = df_team)
```

Might be a relationship between 3P and 2p?

```{r VIF PG}
car::vif(fit_pg)
```

#There is a lot of multicollinearity seen. There is large variation between 3P and 2P. 

```{r multicollinearity MP}
pairs(formula= ~ x3P_pmp + x2P_pmp + FT_pmp, data = df_team)
```

# No relationship seen. 

```{r VIF MP}
car::vif(fit_mp)
```

## Predicted values.
# So based on this we have chosen 'fit_mp' as our predicted model.

```{r predict MP}
head(predict(fit_mp))
```

```{r summary MP}
summary(fit_mp)
```

##Predict model testing
Use the predicted wins (PW) based on the data from Stats team


```{r predicted wins ceofficient}
exp_PTS_pmp = predict(fit_mp)
coef(lm(PW ~ exp_PTS_pmp, data = df_team)) # used this to find the slope and coefficient of these two variables

```

```{r predicted wins MP}

ggplot(df_team, aes(exp_PTS_pmp, PW, label = Team)) +
  geom_point(colour = "dodgerblue") +
  geom_text(nudge_x = 0.005, cex = 3) +
  geom_abline(linetype = "dashed", colour = "red", intercept = -171, slope = 460.8762  )
```


# Against actual wins

```{r predicted wins ceofficient}
exp_PTS_pmp = predict(fit_mp)
coef(lm(W ~ exp_PTS_pmp, data = df_team)) # used this to find the slope and coefficient of these two variables

```

```{r predicted wins MP}
ggplot(df_team, aes(exp_PTS_pmp, W, label = Team)) +
  geom_point(colour = "dodgerblue") +
  geom_text(nudge_x = 0.005, nudge_y = 1, cex = 3) +
  geom_abline(linetype = "dashed", colour = "red", intercept = -177, slope = 475.6097  )
```

###For the majoirty of teams this metric shows that expected points per minute will correlate with a win. 


